{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bc1b08b",
   "metadata": {},
   "source": [
    "# [M_04] PROTOK√ì≈Å: PAMIƒòƒÜ D≈ÅUGOTRWA≈ÅA (QDRANT RAG)\n",
    "\n",
    "**PROJEKT:** OMNI-OPERATOR-V1  \n",
    "**SILNIK:** GEMINI 2.5 FLASH + TEXT-EMBEDDING-004  \n",
    "**STATUS:** IMPLEMENTACJA_RECOGNITION\n",
    "\n",
    "Ten modu≈Ç odpowiada za budowƒô \"pamiƒôci strategicznej\" systemu. Zapisujemy wyniki analiz (M_01) oraz wygenerowane posty (M_02) w wektorowej bazie danych **Qdrant**.\n",
    "\n",
    "**Dlaczego to robimy?**\n",
    "1. **Analiza Stylu:** System mo≈ºe sprawdziƒá, jakie hooki stosowa≈Ç w przesz≈Ço≈õci.\n",
    "2. **Unikanie Powt√≥rze≈Ñ:** Agent wie, o czym ju≈º pisa≈Ç, aby nie powielaƒá tre≈õci.\n",
    "3. **Sovereign Data:** Twoja wiedza o tym, co \"dzia≈Ça\", zostaje na Twoim serwerze w Dockerze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d10fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\takze\\OneDrive\\Pulpit\\project\\omni-operator-v1\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\takze\\AppData\\Local\\Temp\\ipykernel_36992\\930723068.py:6: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: System pamiƒôci gotowy. Po≈ÇƒÖczono z Qdrant na http://localhost:6333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\takze\\AppData\\Local\\Temp\\ipykernel_36992\\930723068.py:19: UserWarning: Qdrant client version 1.16.2 is incompatible with server version 1.12.0. Major versions should match and minor version difference must not exceed 1. Set check_compatibility=False to skip version check.\n",
      "  qclient = QdrantClient(url=settings.qdrant_url)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "import google.generativeai as genai\n",
    "\n",
    "# 1. KOREKTA ≈öCIE≈ªKI ROBOCZEJ\n",
    "if os.getcwd().endswith(\"notebooks\"):\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "# Dodanie src do path\n",
    "sys.path.append(os.path.join(os.getcwd(), \"src\"))\n",
    "\n",
    "from src.core.config import settings\n",
    "\n",
    "# 2. INICJALIZACJA KLIENT√ìW\n",
    "# Qdrant dzia≈Ça na porcie 6333 w Twoim Dockerze\n",
    "qclient = QdrantClient(url=settings.qdrant_url)\n",
    "\n",
    "# Konfiguracja Google do Embedding√≥w (zamiana tekstu na wektory)\n",
    "genai.configure(api_key=settings.gemini_api_key)\n",
    "\n",
    "print(f\"LOG: System pamiƒôci gotowy. Po≈ÇƒÖczono z Qdrant na {settings.qdrant_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7c72a4",
   "metadata": {},
   "source": [
    "## 1. Tworzenie Kolekcji (Schema bazy wektorowej)\n",
    "\n",
    "Definiujemy kolekcjƒô `content_memory`. Rozmiar wektora (768) odpowiada najnowszemu modelowi Google `text-embedding-004`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd4978cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Kolekcja content_memory ju≈º istnieje.\n"
     ]
    }
   ],
   "source": [
    "COLLECTION_NAME = \"content_memory\"\n",
    "\n",
    "def init_memory():\n",
    "    \"\"\"Tworzy kolekcjƒô w Qdrant, je≈õli jeszcze nie istnieje.\"\"\"\n",
    "    collections = qclient.get_collections().collections\n",
    "    exists = any(c.name == COLLECTION_NAME for c in collections)\n",
    "    \n",
    "    if not exists:\n",
    "        print(f\"LOG: Tworzƒô nowƒÖ kolekcjƒô: {COLLECTION_NAME}\")\n",
    "        qclient.create_collection(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    "        )\n",
    "        print(\"‚úÖ KOLEKCJA UTWORZONA\")\n",
    "    else:\n",
    "        print(f\"LOG: Kolekcja {COLLECTION_NAME} ju≈º istnieje.\")\n",
    "\n",
    "init_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119fe8bd",
   "metadata": {},
   "source": [
    "## 2. Funkcja Osadzania (Embedding)\n",
    "\n",
    "Zamieniamy ludzki tekst na listƒô liczb (wektor), kt√≥rƒÖ AI potrafi por√≥wnywaƒá matematycznie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ecaf7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Wygenerowano wektor o d≈Çugo≈õci: 768\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text: str):\n",
    "    \"\"\"Generuje wektor dla podanego tekstu przy u≈ºyciu modelu Google.\"\"\"\n",
    "    result = genai.embed_content(\n",
    "        model=\"models/text-embedding-004\",\n",
    "        content=text,\n",
    "        task_type=\"retrieval_document\"\n",
    "    )\n",
    "    return result['embedding']\n",
    "\n",
    "# TEST:\n",
    "sample_vec = get_embedding(\"In≈ºynieria AI w Ku≈∫ni Operator√≥w\")\n",
    "print(f\"LOG: Wygenerowano wektor o d≈Çugo≈õci: {len(sample_vec)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e83a0ab",
   "metadata": {},
   "source": [
    "## 3. Zapisywanie Kampanii do Pamiƒôci\n",
    "\n",
    "Implementujemy logikƒô \"zapamiƒôtywania\". Zapisujemy tre≈õƒá kampanii wraz z jej metadanymi (temat, platformy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09955f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Generowanie embeddingu dla tematu: Suwerenno≈õƒá AI 2026...\n",
      "LOG: Wysy≈Çanie do Qdrant (ID: dcb746ce-16e0-4c16-8be1-56007ccaf4a8)...\n",
      "‚úÖ ZAPAMIƒòTANO POMY≈öLNIE: Suwerenno≈õƒá AI 2026\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def save_campaign_to_memory(brief_data: dict, topic: str):\n",
    "    \"\"\"Zapisuje raport kampanii do bazy Qdrant.\"\"\"\n",
    "    \n",
    "    print(f\"LOG: Generowanie embeddingu dla tematu: {topic}...\")\n",
    "    # 1. Pobieramy wektor\n",
    "    content_to_embed = f\"Topic: {topic}. Strategy: {brief_data['overall_strategy']}\"\n",
    "    vector = get_embedding(content_to_embed)\n",
    "    \n",
    "    # 2. Przygotowujemy punkt danych\n",
    "    point_id = str(uuid.uuid4())\n",
    "    \n",
    "    # U≈ºywamy natywnego datetime zamiast os.popen('date')\n",
    "    timestamp_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    print(f\"LOG: Wysy≈Çanie do Qdrant (ID: {point_id})...\")\n",
    "    \n",
    "    # 3. Zapis punktu do bazy\n",
    "    qclient.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=[\n",
    "            PointStruct(\n",
    "                id=point_id,\n",
    "                vector=vector,\n",
    "                payload={\n",
    "                    \"topic\": topic,\n",
    "                    \"strategy\": brief_data['overall_strategy'],\n",
    "                    \"type\": \"campaign_brief\",\n",
    "                    \"timestamp\": timestamp_str\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    print(f\"‚úÖ ZAPAMIƒòTANO POMY≈öLNIE: {topic}\")\n",
    "\n",
    "# TEST: Ponowne uruchomienie\n",
    "mock_brief = {\n",
    "    \"overall_strategy\": \"Pozycjonowanie Takzen Dev jako lidera suwerennego AI.\",\n",
    "    \"posts\": []\n",
    "}\n",
    "save_campaign_to_memory(mock_brief, \"Suwerenno≈õƒá AI 2026\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e879212",
   "metadata": {},
   "source": [
    "## 4. Wyszukiwanie Semantyczne (RAG Test)\n",
    "\n",
    "Sprawdzamy, czy system potrafi odnale≈∫ƒá powiƒÖzane tre≈õci bez u≈ºycia s≈Ç√≥w kluczowych, bazujƒÖc na samym \"sensie\" zapytania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "985fc493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Generowanie embeddingu dla zapytania: 'Szukam czego≈õ o niezale≈ºno≈õci od SaaS'...\n",
      "LOG: Przeszukiwanie bazy wektorowej...\n",
      "\n",
      "üîé WYNIKI DLA: 'Szukam czego≈õ o niezale≈ºno≈õci od SaaS'\n",
      "----------------------------------------\n",
      " -> [Zgodno≈õƒá: 0.74] Suwerenno≈õƒá AI 2026\n",
      "    Strategia: Pozycjonowanie Takzen Dev jako lidera suwerennego AI....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search_memory(query: str, limit: int = 2):\n",
    "    \"\"\"\n",
    "    Przeszukuje pamiƒôƒá przy u≈ºyciu nowoczesnego API Qdrant 1.16+.\n",
    "    \"\"\"\n",
    "    print(f\"LOG: Generowanie embeddingu dla zapytania: '{query}'...\")\n",
    "    query_vector = get_embedding(query)\n",
    "    \n",
    "    print(\"LOG: Przeszukiwanie bazy wektorowej...\")\n",
    "    \n",
    "    # U≈ºywamy najnowszego API query_points (Standard 2025/2026)\n",
    "    hits = qclient.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query=query_vector,\n",
    "        limit=limit,\n",
    "        with_payload=True\n",
    "    ).points\n",
    "    \n",
    "    if not hits:\n",
    "        print(\"‚ÑπÔ∏è Brak pasujƒÖcych wspomnie≈Ñ w bazie.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nüîé WYNIKI DLA: '{query}'\")\n",
    "    print(\"-\" * 40)\n",
    "    for hit in hits:\n",
    "        score = hit.score\n",
    "        topic = hit.payload.get('topic', 'Brak tematu')\n",
    "        strategy = hit.payload.get('strategy', '')[:100]\n",
    "        print(f\" -> [Zgodno≈õƒá: {score:.2f}] {topic}\")\n",
    "        print(f\"    Strategia: {strategy}...\\n\")\n",
    "\n",
    "# TEST OPERACYJNY:\n",
    "search_memory(\"Szukam czego≈õ o niezale≈ºno≈õci od SaaS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6b9c4",
   "metadata": {},
   "source": [
    "## STATUS: MODU≈Å 04 ZAKO≈ÉCZONY\n",
    "\n",
    "Mamy w pe≈Çni funkcjonalnƒÖ pamiƒôƒá wektorowƒÖ. Tw√≥j system potrafi teraz gromadziƒá do≈õwiadczenie.\n",
    "\n",
    "**OsiƒÖgniƒôcia:**\n",
    "1. Integracja lokalnego Qdranta z Embeddingami Google.\n",
    "2. Mo≈ºliwo≈õƒá zapisu i odzyskiwania wiedzy (RAG).\n",
    "3. Podwaliny pod Etap 5 (Dyrygent), gdzie system bƒôdzie sprawdza≈Ç bazƒô przed ka≈ºdym monta≈ºem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omni-operator-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
