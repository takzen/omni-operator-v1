{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bc1b08b",
   "metadata": {},
   "source": [
    "# [M_04] PROTOCOL: LONG-TERM MEMORY (QDRANT RAG)\n",
    "\n",
    "**PROJECT:** OMNI-OPERATOR-V1  \n",
    "**ENGINE:** GEMINI 3 FLASH + TEXT-EMBEDDING-004  \n",
    "**STATUS:** RECOGNITION_IMPLEMENTATION\n",
    "\n",
    "This module is responsible for building the system's \"strategic memory\". We save analysis results (M_01) and generated posts (M_02) in **Qdrant** vector database.\n",
    "\n",
    "**Why are we doing this?**\n",
    "1. **Style Analysis:** System can check what hooks it used in the past.\n",
    "2. **Avoiding Repetition:** Agent knows what it already wrote about, to avoid duplicating content.\n",
    "3. **Sovereign Data:** Your knowledge about what \"works\" stays on your server in Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d10fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Memory system ready. Connected to Qdrant at http://localhost:6333\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "from google import genai\n",
    "\n",
    "# 1. WORKING DIRECTORY CORRECTION\n",
    "if os.getcwd().endswith(\"notebooks\"):\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "# Adding src to path \n",
    "sys.path.append(os.path.join(os.getcwd(), \"src\"))\n",
    "\n",
    "from src.core.config import settings\n",
    "\n",
    "# 2. CLIENT INITIALIZATION\n",
    "# Qdrant is running on port 6333 in Docker\n",
    "qclient = QdrantClient(url=settings.qdrant_url, timeout=60)\n",
    "\n",
    "# Google configuration for embeddings (converting text to vectors)\n",
    "client = genai.Client(api_key=settings.gemini_api_key)\n",
    "\n",
    "print(f\"LOG: Memory system ready. Connected to Qdrant at {settings.qdrant_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7c72a4",
   "metadata": {},
   "source": [
    "## 1. Creating a Collection (Vector Database Schema)\n",
    "\n",
    "We define the `content_memory` collection. The vector size (768) corresponds to the latest Google `text-embedding-004` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd4978cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Collection content_memory already exists.\n"
     ]
    }
   ],
   "source": [
    "COLLECTION_NAME = \"content_memory\"\n",
    "\n",
    "def init_memory():\n",
    "    \"\"\"Creates a collection in Qdrant if it doesn't already exist.\"\"\"\n",
    "    collections = qclient.get_collections().collections\n",
    "    exists = any(c.name == COLLECTION_NAME for c in collections)\n",
    "    \n",
    "    if not exists:\n",
    "        print(f\"LOG: Creating new collection: {COLLECTION_NAME}\")\n",
    "        qclient.create_collection(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    "        )\n",
    "        print(\"âœ… COLLECTION CREATED\")\n",
    "    else:\n",
    "        print(f\"LOG: Collection {COLLECTION_NAME} already exists.\")\n",
    "\n",
    "init_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119fe8bd",
   "metadata": {},
   "source": [
    "## 2. Embedding Function\n",
    "\n",
    "We convert human text into a list of numbers (a vector) that AI can compare mathematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ecaf7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Generated vector length: 768\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text: str):\n",
    "    result = client.models.embed_content(\n",
    "        model=\"text-embedding-004\",\n",
    "        contents=text\n",
    "    )\n",
    "    return result.embeddings[0].values\n",
    "\n",
    "# TEST:\n",
    "sample_vec = get_embedding(\"AI Engineering at KuÅºnia OperatorÃ³w\")\n",
    "print(f\"LOG: Generated vector length: {len(sample_vec)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e83a0ab",
   "metadata": {},
   "source": [
    "## 3. Saving Campaigns to Memory\n",
    "\n",
    "Implementing the \"memorization\" logic. We save the campaign content along with its metadata (topic, platforms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09955f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Generating embedding for topic: AI Sovereignty 2026...\n",
      "LOG: Sending to Qdrant (ID: af02bb5a-aa3a-4411-87bd-2df9e418e559) with clip metadata...\n",
      "âœ… SUCCESSFULLY REMEMBERED: AI Sovereignty 2026 (2 clips)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def save_campaign_to_memory(brief_data: dict, topic: str):\n",
    "    \"\"\"Saves the campaign report to the Qdrant database, including clip metadata.\"\"\"\n",
    "    \n",
    "    print(f\"LOG: Generating embedding for topic: {topic}...\")\n",
    "    # 1. Retrieve the vector (based on the topic and general strategy)\n",
    "    content_to_embed = f\"Topic: {topic}. Strategy: {brief_data['overall_strategy']}\"\n",
    "    vector = get_embedding(content_to_embed)\n",
    "    \n",
    "    # 2. Prepare the data point\n",
    "    point_id = str(uuid.uuid4())\n",
    "    timestamp_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Extraction of clip metadata (durations and hooks)\n",
    "    clips_meta = [\n",
    "        {\n",
    "            \"idx\": c.get('clip_index'),\n",
    "            \"duration\": c.get('duration_seconds'),\n",
    "            \"hook_sample\": c.get('posts', [{}])[0].get('content', '')[:50]\n",
    "        } for c in brief_data.get('clip_strategies', [])\n",
    "    ]\n",
    "    \n",
    "    print(f\"LOG: Sending to Qdrant (ID: {point_id}) with clip metadata...\")\n",
    "    \n",
    "    # 3. Save the point to the database\n",
    "    qclient.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=[\n",
    "            PointStruct(\n",
    "                id=point_id,\n",
    "                vector=vector,\n",
    "                payload={\n",
    "                    \"topic\": topic,\n",
    "                    \"strategy\": brief_data['overall_strategy'],\n",
    "                    \"clips\": clips_meta, # <--- This is where our new knowledge about duration goes\n",
    "                    \"type\": \"campaign_brief\",\n",
    "                    \"timestamp\": timestamp_str\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    print(f\"âœ… SUCCESSFULLY REMEMBERED: {topic} ({len(clips_meta)} clips)\")\n",
    "\n",
    "# TEST COMPLIANT WITH THE NEW STANDARD:\n",
    "mock_brief = {\n",
    "    \"overall_strategy\": \"Positioning Takzen Dev as a leader in sovereign AI.\",\n",
    "    \"clip_strategies\": [\n",
    "        {\"clip_index\": 1, \"duration_seconds\": 25, \"posts\": [{\"content\": \"Eliminating SaaS...\"}]},\n",
    "        {\"clip_index\": 2, \"duration_seconds\": 45, \"posts\": [{\"content\": \"Private Docker...\"}]}\n",
    "    ]\n",
    "}\n",
    "save_campaign_to_memory(mock_brief, \"AI Sovereignty 2026\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e879212",
   "metadata": {},
   "source": [
    "## 4. Semantic Search (RAG Test)\n",
    "\n",
    "We are checking if the system can find related content without using keywords, based solely on the \"meaning\" of the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "985fc493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Generating embedding for query: 'Looking for something about SaaS independence'...\n",
      "LOG: Searching vector database...\n",
      "\n",
      "ðŸ”Ž RESULTS FOR: 'Looking for something about SaaS independence'\n",
      "----------------------------------------\n",
      " -> [Match: 0.38] AI Sovereignty 2026\n",
      "    Strategy: Positioning Takzen Dev as a leader in sovereign AI....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search_memory(query: str, limit: int = 2):\n",
    "    \"\"\"\n",
    "    Searches memory using modern Qdrant API 1.16+.\n",
    "    \"\"\"\n",
    "    print(f\"LOG: Generating embedding for query: '{query}'...\")\n",
    "    query_vector = get_embedding(query)\n",
    "    \n",
    "    print(\"LOG: Searching vector database...\")\n",
    "    \n",
    "    # Using the latest query_points API (2025/2026 Standard)\n",
    "    hits = qclient.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query=query_vector,\n",
    "        limit=limit,\n",
    "        with_payload=True\n",
    "    ).points\n",
    "    \n",
    "    if not hits:\n",
    "        print(\"â„¹ï¸ No matching memories found in the database.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nðŸ”Ž RESULTS FOR: '{query}'\")\n",
    "    print(\"-\" * 40)\n",
    "    for hit in hits:\n",
    "        score = hit.score\n",
    "        topic = hit.payload.get('topic', 'No topic')\n",
    "        strategy = hit.payload.get('strategy', '')[:100]\n",
    "        print(f\" -> [Match: {score:.2f}] {topic}\")\n",
    "        print(f\"    Strategy: {strategy}...\\n\")\n",
    "\n",
    "# OPERATIONAL TEST:\n",
    "search_memory(\"Looking for something about SaaS independence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6b9c4",
   "metadata": {},
   "source": [
    "## STATUS: MODULE 04 COMPLETED\n",
    "\n",
    "We have a fully functional vector memory. Your system is now capable of gathering experience.\n",
    "\n",
    "**Achievements:**\n",
    "1. Integration of local Qdrant with Google Embeddings.\n",
    "2. Ability to store and retrieve knowledge (RAG).\n",
    "3. Foundations for Phase 5 (Conductor), where the system will check the database before every edit/assembly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omni-operator-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
